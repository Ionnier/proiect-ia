{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyphen nltk pandas sklearn pronouncing\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "#! rm -f dale_chall.py\n",
    "#! wget https://raw.githubusercontent.com/artificial-intelligence-ml-cti/ml_cti/main/proiect/dale_chall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dale_chall import DALE_CHALL\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citire date\n",
    "dtypes = {\"sentence\": \"string\", \"token\": \"string\", \"complexity\": \"float64\"}\n",
    "train = pd.read_excel(os.path.join(os.getcwd(), \"data\",\n",
    "                      \"train.xlsx\"), dtype=dtypes, keep_default_na=False)\n",
    "test = pd.read_excel(os.path.join(os.getcwd(), \"data\",\n",
    "                     \"test.xlsx\"), dtype=dtypes, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma date de test si antrenare\n",
    "print('train data: ', train.shape)\n",
    "print('test data: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nr_syllables(word):\n",
    "    return len(pyphen.Pyphen(lang=\"en\").inserted(word,\"-\").split(\"-\"))\n",
    "\n",
    "def is_dale_chall(word):\n",
    "        return (word.lower() in DALE_CHALL) or (WordNetLemmatizer().lemmatize(word, pos=\"n\") in DALE_CHALL) or (WordNetLemmatizer().lemmatize(word, pos=\"v\") in DALE_CHALL) or (WordNetLemmatizer().lemmatize(word, pos=\"a\") in DALE_CHALL) or (WordNetLemmatizer().lemmatize(word, pos=\"r\") in DALE_CHALL) or (WordNetLemmatizer().lemmatize(word, pos=\"s\") in DALE_CHALL)\n",
    "\n",
    "\n",
    "def length(word):\n",
    "    return len(word.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def nr_vowels(word):\n",
    "    vowels = [\"a\", \"e\", \"o\", \"u\",\n",
    "     \"i\", \"y\"]\n",
    "    nr_vowels = 0\n",
    "    word = word.lower()\n",
    "    for vowel in vowels:\n",
    "        nr_vowels += word.count(vowel)\n",
    "    return nr_vowels\n",
    "\n",
    "def nr_consoane(word):\n",
    "    vowels = [\"b\",\"c\",\"d\",\"f\",\"g\",\"h\",\"j\",\"k\",\"l\",\"m\",\"n\",\"p\",\"q\",\"r\",\"s\",\"t\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "    nr_cons = 0\n",
    "    word = word.lower()\n",
    "    for vowel in vowels:\n",
    "        nr_cons += word.count(vowel)\n",
    "    return nr_cons\n",
    "\n",
    "\n",
    "def is_title(word):\n",
    "    if word == word.capitalize():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def abreviation(word):\n",
    "    if word == word.upper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def repeating_characters(word):\n",
    "    chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    word = word.lower()\n",
    "    characters = 0\n",
    "    for char in chars:\n",
    "        count = word.count(char)\n",
    "        if count > 1:\n",
    "            characters += 1\n",
    "    return characters == 0\n",
    "\n",
    "\n",
    "text = [ x.strip().encode(\"ascii\", \"ignore\") for x in list(train['sentence'])]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text)\n",
    "vectorizer.get_feature_names_out()\n",
    "df_tfid = pd.DataFrame(data = X.toarray(),columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "def get_tfid(word):\n",
    "    try:\n",
    "        li = list(df_tfid[word])\n",
    "        return sum(li)/len(li)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_word_structure_features(word):\n",
    "    features = []\n",
    "    features.append(nr_syllables(word))\n",
    "    features.append(is_dale_chall(word))\n",
    "    features.append(length(word))\n",
    "    features.append(nr_vowels(word))\n",
    "    features.append(is_title(word))\n",
    "    features.append(abreviation(word))\n",
    "    features.append(prononciation(word))\n",
    "    #features.append(get_tfid(word))\n",
    "    features.append(repeating_characters(word))\n",
    "    features.append(nr_consoane(word))\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def get_wordnet_features(word):\n",
    "    features = []\n",
    "    features.append(len(wordnet.synsets(word)))\n",
    "    try:\n",
    "        len(wordnet.synsets(word)[0].definition())\n",
    "        features.append(1)\n",
    "    except:\n",
    "        features.append(0)\n",
    "    try:\n",
    "        len(wordnet.synsets(word)[0].examples())\n",
    "        features.append(1)\n",
    "    except:\n",
    "        features.append(0)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def get_position_in_sentence(sentence, word):\n",
    "    try:\n",
    "        cp_sentence = sentence.lower()\n",
    "        cp_word = word.lower()\n",
    "        words = re.findall(r'\\w+', cp_sentence)\n",
    "        position = words.index(cp_word)\n",
    "        return [int(position/len(words)) * 10]\n",
    "    except ValueError:\n",
    "        words = sentence.split(' ')\n",
    "        for x in range(len(words)):\n",
    "            if word in words[x]:\n",
    "                return [int(x/len(words)) * 10]\n",
    "\n",
    "\n",
    "def complex_sentence(sentence):\n",
    "    if sentence.strip().encode(\"ascii\", \"ignore\") == sentence:\n",
    "        return [True]\n",
    "    return [False]\n",
    "\n",
    "\n",
    "def mean_complexity_sentence(sentence):\n",
    "    array = []\n",
    "    for x in word_tokenize(sentence):\n",
    "        array.append(get_word_structure_features(x))\n",
    "    return [np.array(array).mean()]\n",
    "\n",
    "\n",
    "def unique_words(sentence):\n",
    "    dictionar = {}\n",
    "    for x in word_tokenize(sentence):\n",
    "        try:\n",
    "            dictionar[x] += 1\n",
    "        except:\n",
    "            dictionar[x] = 1\n",
    "    return [len(dictionar)]\n",
    "\n",
    "\n",
    "def corpus_feature(corpus):\n",
    "    corp_dict = {}\n",
    "    corp_dict['bible'] = [0]\n",
    "    corp_dict['europarl'] = [1]\n",
    "    corp_dict['biomed'] = [2]\n",
    "    return corp_dict[corpus]\n",
    "    \n",
    "def prononciation(word):\n",
    "    try:\n",
    "        pronouncing.phones_for_word(word)[0].split()\n",
    "        return 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "row = train.iloc[:1]\n",
    "# print(corpus_feature(row['corpus'])[0])\n",
    "\n",
    "\n",
    "def featurize(row):\n",
    "    word = row['token']\n",
    "    all_features = []\n",
    "    all_features.extend(corpus_feature(row['corpus']))\n",
    "    all_features.extend(get_word_structure_features(word))\n",
    "    all_features.extend(get_wordnet_features(word))\n",
    "    all_features.extend(complex_sentence(row['sentence']))\n",
    "    #all_features.extend(mean_complexity_sentence(row['sentence']))\n",
    "    all_features.extend(unique_words(row['sentence'])) #extremly unimportant\n",
    "    all_features.extend(get_position_in_sentence(row['sentence'], word))\n",
    "    return np.array(all_features)\n",
    "\n",
    "\n",
    "def featurize_df(df):\n",
    "    nr_of_features = len(featurize(df.iloc[0]))\n",
    "    nr_of_examples = len(df)\n",
    "    features = np.zeros((nr_of_examples, nr_of_features))\n",
    "    for index, row in df.iterrows():\n",
    "        row_ftrs = featurize(row)\n",
    "        features[index, :] = row_ftrs\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_of_indexes = []\n",
    "for x in train[\"corpus\"].unique():\n",
    "    arrays_of_indexes.append(train.loc[train['corpus'] == x].index)\n",
    "\n",
    "\n",
    "def generate_data(percentage=15):\n",
    "    chosen_idx = []\n",
    "    for array in arrays_of_indexes:\n",
    "        chosen_idx.extend(np.random.choice(\n",
    "            array, replace=False, size=int(percentage/100*len(array))))\n",
    "    new_test = train.iloc[chosen_idx]\n",
    "    new_train = train.drop(chosen_idx)\n",
    "    new_train.reset_index(drop=True, inplace=True)\n",
    "    new_test.reset_index(drop=True, inplace=True)\n",
    "    X_train = featurize_df(new_train)\n",
    "    y_train = new_train['complex'].values\n",
    "    X_test = featurize_df(new_test)\n",
    "    y_test = new_test['complex'].values\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predicitons(X_train, y_train, X_test, neighbours):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbours)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-075539537065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mscor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_predicitons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mscor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbalanced_accuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-32bfee4bb369>\u001b[0m in \u001b[0;36mgenerate_data\u001b[1;34m(percentage)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnew_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnew_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'complex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5b26d1faa397>\u001b[0m in \u001b[0;36mfeaturize_df\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mrow_ftrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_ftrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for nb in [1, 3, 5, 7]:\n",
    "    scor = []\n",
    "    for _ in range(20):\n",
    "        X_train, y_train, X_test, y_test = generate_data(15)\n",
    "        preds = knn_predicitons(X_train, y_train, X_test, nb)\n",
    "        scor.append(balanced_accuracy_score(y_test, preds))\n",
    "    print(nb, np.array(scor).mean())\n",
    "\n",
    "\"\"\"\n",
    "1 0.6283665173443896\n",
    "3 0.5996178096129835\n",
    "5 0.5849402420262173\n",
    "7 0.5858485635024839\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb in [1, 3, 5, 7]:\n",
    "    scor = []\n",
    "    for _ in range(20):\n",
    "        X_train, y_train, X_test, y_test = generate_data(15)\n",
    "        model = KNeighborsClassifier(n_neighbors=nb)\n",
    "        preds = model.fit(X_train, y_train).predict(X_test)\n",
    "        scor.append(balanced_accuracy_score(y_test, preds))\n",
    "    print(nb, np.array(scor).mean())\n",
    "\"\"\"\n",
    "Cu Scale(4m 9s)\n",
    "1 0.620526913251418\n",
    "3 0.5926311831865265\n",
    "5 0.6045628701428246\n",
    "7 0.5900995288967413\n",
    "\n",
    "Fara Scale(4m 6s)\n",
    "1 0.6278159750606886\n",
    "3 0.5950050152182468\n",
    "5 0.6005444531799113\n",
    "7 0.5951236211015452\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cross_val_preds(preds):\n",
    "    print(\"\\\\begin{center}\")\n",
    "    print(\"\\\\begin{tabular}{||c c c c||}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\Split & Fit Time & Score Time & Test Score \\\\\\\\ [0.5ex]\")\n",
    "    for i in range(len(preds['fit_time'])):\n",
    "        print(\"\\\\hline\")\n",
    "        fit_time = preds['fit_time'][i]\n",
    "        score_time = preds['score_time'][i]\n",
    "        test_score = preds['test_score'][i]\n",
    "        print(f\"{i+1} & {fit_time} & {score_time} & {test_score} \\\\\\\\\")\n",
    "        print(\"\\\\hline\")\n",
    "        i += 1\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\end{center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nb in [1, 3, 5, 7]:\n",
    "    X_train = featurize_df(train)\n",
    "    y_train = train['complex'].values\n",
    "    model = KNeighborsClassifier(n_neighbors=nb)\n",
    "    model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=nb))\n",
    "    preds = cross_validate(model, X_train, y_train, cv=10, scoring='balanced_accuracy')\n",
    "    print(nb, sum(preds[\"test_score\"])/len(preds[\"test_score\"]))\n",
    "    #print(preds)\n",
    "# Fara Scale\n",
    "# 1 0.5846458931095925\n",
    "# 3 0.5879787510212504\n",
    "# 5 0.5951311466724666\n",
    "# 7 0.5962992925837006\n",
    "\n",
    "# Cu Scale\n",
    "# 1 0.5998953946836425\n",
    "# 3 0.6193787354062276\n",
    "# 5 0.6180504588307136\n",
    "# 7 0.620784033081541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "#preds = cross_validate(model, X_train, y_train, cv=10, scoring='balanced_accuracy')\n",
    "#print_cross_val_preds(preds)\n",
    "print(3, sum(preds[\"test_score\"])/len(preds[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:06.009618\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "X_train = featurize_df(train)\n",
    "y_train = train['complex'].values\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6789562891242662\n",
      "[[1338   61]\n",
      " [  79   53]]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = generate_data(20)\n",
    "model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "preds = model.fit(X_train, y_train).predict(X_test)\n",
    "print(balanced_accuracy_score(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "print(sum(preds==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = featurize_df(train)\n",
    "y_train = train['complex'].values\n",
    "X_test = featurize_df(test)\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test.index + len(train) + 1\n",
    "df['complex'] = knn_predicitons(X_train, y_train, X_test, neighbours=3)\n",
    "df.to_csv('submission.csv', index=False)\n",
    "# Fara Scale Public 0.56556 Private 0.59246\n",
    "# Cu Scale Public 0.59350 Private 0.64657\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b455c340f5909324bcb78ef01d475f4eedf82c96c915f98f0f532100f1af7503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
