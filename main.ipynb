{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# !pip install pyphen nltk pandas sklearn\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "import cupy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from dale_chall import DALE_CHALL\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"sentence\": \"string\", \"token\": \"string\", \"complexity\": \"float64\"}\n",
    "train = pd.read_excel(os.path.join(os.getcwd(), \"data\",\n",
    "                      \"train.xlsx\"), dtype=dtypes, keep_default_na=False)\n",
    "test = pd.read_excel(os.path.join(os.getcwd(), \"data\",\n",
    "                     \"test.xlsx\"), dtype=dtypes, keep_default_na=False)\n",
    "#print('train data: ', train.shape)\n",
    "#print('test data: ', test.shape)\n",
    "\n",
    "def accuracy(y_true: np.array, y_pred: np.array):\n",
    "    return np.sum((y_true == y_pred).astype(int)) / len(y_true) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"complex\"] == False].to_csv(\"simple.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nr_syllables(word):\n",
    "    import re\n",
    "    return len(\n",
    "        re.findall('(?!e$)[aeiouy]+', word, re.I) +\n",
    "        re.findall('^[^aeiouy]*e$', word, re.I)\n",
    "    )\n",
    "\n",
    "\n",
    "def is_dale_chall(word):\n",
    "        return (word.lower() in DALE_CHALL)\n",
    "\n",
    "\n",
    "def length(word):\n",
    "    return len(word.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def nr_vowels(word):\n",
    "    vowels = [\"a\", \"e\", \"o\", \"u\", \"i\", \"y\"]\n",
    "    nr_vowels = 0\n",
    "    word = word.lower()\n",
    "    for vowel in vowels:\n",
    "        nr_vowels += word.count(vowel)\n",
    "    return nr_vowels\n",
    "\n",
    "def nr_consoane(word):\n",
    "    vowels = [\"b\",\"c\",\"d\",\"f\",\"g\",\"h\",\"j\",\"k\",\"l\",\"m\",\"n\",\"p\",\"q\",\"r\",\"s\",\"t\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "    nr_cons = 0\n",
    "    word = word.lower()\n",
    "    for vowel in vowels:\n",
    "        nr_cons += word.count(vowel)\n",
    "    return nr_cons\n",
    "\n",
    "\n",
    "def is_title(word):\n",
    "    if word == word.capitalize():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def abreviation(word):\n",
    "    if word == word.upper():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def repeating_characters(word):\n",
    "    chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    word = word.lower()\n",
    "    characters = 0\n",
    "    for char in chars:\n",
    "        count = word.count(char)\n",
    "        if count > 1:\n",
    "            characters += 1\n",
    "    return characters\n",
    "\n",
    "\n",
    "def get_word_structure_features(word):\n",
    "    features = []\n",
    "    features.append(nr_syllables(word))\n",
    "    features.append(is_dale_chall(word))\n",
    "    features.append(length(word))\n",
    "    features.append(nr_vowels(word))\n",
    "    features.append(is_title(word))\n",
    "    features.append(abreviation(word))\n",
    "    features.append(repeating_characters(word))\n",
    "    features.append(nr_consoane(word))\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def get_wordnet_features(word):\n",
    "    features = []\n",
    "    features.append(len(wordnet.synsets(word)))\n",
    "    # try:\n",
    "    #     features.append(len(wordnet.synsets(word)[0].definition()))\n",
    "    # except:\n",
    "    #     features.append(0)\n",
    "    try:\n",
    "        features.append(len(wordnet.synsets(word)[0].examples()))\n",
    "    except:\n",
    "        features.append(0)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def get_position_in_sentence(sentence, word):\n",
    "    try:\n",
    "        cp_sentence = sentence.lower()\n",
    "        cp_word = word.lower()\n",
    "        words = re.findall(r'\\w+', cp_sentence)\n",
    "        position = words.index(cp_word)\n",
    "        return [int(position/len(words)) * 10]\n",
    "    except ValueError:\n",
    "        words = sentence.split(' ')\n",
    "        for x in range(len(words)):\n",
    "            if word in words[x]:\n",
    "                return [int(x/len(words)) * 10]\n",
    "\n",
    "\n",
    "def complex_sentence(sentence):\n",
    "    for x in sentence:\n",
    "        if(ord(x) > 127):\n",
    "            return [True]\n",
    "    return [False]\n",
    "\n",
    "\n",
    "def mean_complexity_sentence(sentence):\n",
    "    array = []\n",
    "    for x in word_tokenize(sentence):\n",
    "        array.append(get_word_structure_features(x))\n",
    "    return [np.array(array).mean()]\n",
    "\n",
    "\n",
    "def unique_words(sentence):\n",
    "    dictionar = {}\n",
    "    for x in word_tokenize(sentence):\n",
    "        try:\n",
    "            dictionar[x] += 1\n",
    "        except:\n",
    "            dictionar[x] = 1\n",
    "    return [len(dictionar)]\n",
    "\n",
    "\n",
    "def corpus_feature(corpus):\n",
    "    corp_dict = {}\n",
    "    corp_dict['bible'] = [0]\n",
    "    corp_dict['europarl'] = [1]\n",
    "    corp_dict['biomed'] = [2]\n",
    "    return corp_dict[corpus]\n",
    "\n",
    "\n",
    "row = train.iloc[:1]\n",
    "# print(corpus_feature(row['corpus'])[0])\n",
    "\n",
    "\n",
    "def featurize(row):\n",
    "    word = row['token']\n",
    "    all_features = []\n",
    "    all_features.extend(corpus_feature(row['corpus']))\n",
    "    all_features.extend(get_word_structure_features(word))\n",
    "    all_features.extend(get_wordnet_features(word))\n",
    "    all_features.extend(complex_sentence(row['sentence']))\n",
    "    all_features.extend(mean_complexity_sentence(row['sentence']))\n",
    "    #all_features.extend(unique_words(row['sentence'])) #extremly unimportant\n",
    "    all_features.extend(get_position_in_sentence(row['sentence'], word))\n",
    "    return np.array(all_features)\n",
    "\n",
    "\n",
    "def featurize_df(df):\n",
    "    nr_of_features = len(featurize(df.iloc[0]))\n",
    "    nr_of_examples = len(df)\n",
    "    features = np.zeros((nr_of_examples, nr_of_features))\n",
    "    for index, row in df.iterrows():\n",
    "        row_ftrs = featurize(row)\n",
    "        features[index, :] = row_ftrs\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_of_indexes = []\n",
    "for x in train[\"corpus\"].unique():\n",
    "    arrays_of_indexes.append(train.loc[train['corpus'] == x].index)\n",
    "\n",
    "\n",
    "def generate_data(percentage=15):\n",
    "    chosen_idx = []\n",
    "    for array in arrays_of_indexes:\n",
    "        chosen_idx.extend(np.random.choice(\n",
    "            array, replace=False, size=int(percentage/100*len(array))))\n",
    "    new_test = train.iloc[chosen_idx]\n",
    "    new_train = train.drop(chosen_idx)\n",
    "    new_train.reset_index(drop=True, inplace=True)\n",
    "    new_test.reset_index(drop=True, inplace=True)\n",
    "    X_train = featurize_df(new_train)\n",
    "    y_train = new_train['complex'].values\n",
    "    X_test = featurize_df(new_test)\n",
    "    y_test = new_test['complex'].values\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predicitons(X_train, y_train, X_test, neighbours):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbours)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "\n",
    "for nb in [1, 3, 5, 7]:\n",
    "    scor = []\n",
    "    for _ in range(20):\n",
    "        X_train, y_train, X_test, y_test = generate_data(5)\n",
    "        preds = knn_predicitons(X_train, y_train, X_test, nb)\n",
    "        scor.append(balanced_accuracy_score(y_test, preds))\n",
    "    print(nb, np.array(scor).mean())\n",
    "\n",
    "\"\"\" Submission 3\n",
    "knn 3 neighbours \n",
    "\n",
    "100 random train data of 15% of total\n",
    "\n",
    "features.append(nr_syllables(word))\n",
    "    features.append(is_dale_chall(word)) // without plural bonus\n",
    "    features.append(length(word))\n",
    "    features.append(nr_vowels(word))\n",
    "    features.append(is_title(word))\n",
    "    features.append(abreviation(word))\n",
    "    features.append(repeating_characters(word))\n",
    "\n",
    "default row features\n",
    "\n",
    "1 0.668427876407733\n",
    "2 0.6126221801373262\n",
    "3 0.6765161869511589\n",
    "4 0.6386154857166626\n",
    "5 0.6634404495232458\n",
    "6 0.6355640755172113\n",
    "7 0.6488212568854519\n",
    "8 0.6259146558693992\n",
    "9 0.6397803876068514\n",
    "10 0.6189791332207973\n",
    "11 0.6285574131056153\n",
    "12 0.6162813529025942\n",
    "13 0.6258473169720228\n",
    "14 0.6120803909664643\n",
    "\n",
    "17m 20s\n",
    "\n",
    "\n",
    "Submission 4\n",
    "added plural to dale, very slight increase\n",
    "1 0.668257413183145\n",
    "2 0.6126109670655668\n",
    "3 0.6774073128977224\n",
    "4 0.6339482434381662\n",
    "5 0.6680421902150359\n",
    "6 0.6356128388286275\n",
    "7 0.6540611829398212\n",
    "8 0.6241868282579967\n",
    "9 0.6428127641844336\n",
    "10 0.6171953886598779\n",
    "11 0.6298598263970768\n",
    "12 0.618206900202975\n",
    "13 0.6238640555630808\n",
    "14 0.6136176874161184\n",
    "\n",
    "\n",
    "add position and complex\n",
    "1 0.6775388554327088\n",
    "2 0.6166385320256123\n",
    "3 0.6824216345750901\n",
    "4 0.6372908738125417\n",
    "5 0.6681370214586604\n",
    "6 0.6330719335337368\n",
    "\n",
    "add length of definition\n",
    "1 0.7142974520544537\n",
    "2 0.6392006863590715\n",
    "3 0.7185837970831896\n",
    "4 0.6562322581550614\n",
    "5 0.6767755930256228\n",
    "6 0.638215011403038\n",
    "7 0.6478298861631056\n",
    "8 0.6014318744187663\n",
    "9 0.6113716172856284\n",
    "\n",
    "add mean complexity of each word\n",
    "1 0.6645660311458911\n",
    "3 0.6825219784240113\n",
    "5 0.6532568645394083\n",
    "7 0.6348892638844663\n",
    "\n",
    "remove position\n",
    "1 0.6756303700338743\n",
    "3 0.6742453028316306\n",
    "5 0.6545457828350185\n",
    "7 0.6446028971371262\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.7920212110001233\n",
      "1.25 0.7815414667919489\n",
      "1.5 0.7610300914039232\n",
      "1.75 0.7849349516827386\n",
      "2.0 0.8100121018282405\n",
      "2.25 0.7721580283195297\n",
      "2.5 0.8210802993883255\n",
      "2.75 0.8039274100142552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nC\\n2.0 0.7989526768510238\\n2.05 0.7663162573389564\\n2.0999999999999996 0.8071139172438091\\n2.1499999999999995 0.7817640548070683\\n2.1999999999999993 0.7849235827168971\\n2.249999999999999 0.8002323036630431\\n2.299999999999999 0.8006677849020192\\n2.3499999999999988 0.8026971693074595\\n2.3999999999999986 0.7829305179191633\\n2.4499999999999984 0.8000819070214088\\n2.4999999999999982 0.8144846512576605\\n2.549999999999998 0.7584803934346462\\n2.599999999999998 0.8094465648854962\\n2.6499999999999977 0.8028424302827774\\n2.6999999999999975 0.8026711563768261\\n2.7499999999999973 0.7883496555315228\\n2.799999999999997 0.8169983162406245\\n2.849999999999997 0.8040404040404041\\n2.899999999999997 0.7843753853742755\\n2.9499999999999966 0.7891142191142191\\n2.9999999999999964 0.7970483341584613\\n3.0499999999999963 0.7826048329779673\\n3.099999999999996 0.7996438000040306\\n3.149999999999996 0.8004216348551481\\n3.1999999999999957 0.7975259473811248\\n3.2499999999999956 0.8104505935831237\\n3.2999999999999954 0.7923782416192284\\n3.349999999999995 0.8022344996930633\\n3.399999999999995 0.8079854522454143\\n3.449999999999995 0.8289995549181544\\n3.4999999999999947 0.7932573090463799\\n3.5499999999999945 0.8008438228438228\\n3.5999999999999943 0.809004662004662\\n3.649999999999994 0.8074961878029119\\n3.699999999999994 0.816032837149498\\n3.749999999999994 0.7907520259618133\\n3.7999999999999936 0.8251874293930368\\n3.8499999999999934 0.7792741165234002\\n3.8999999999999932 0.796911421911422\\n3.949999999999993 0.8119436242798217\\n\\ngamma\\n2.0 0.7989526768510238\\n2.05 0.7663162573389564\\n2.0999999999999996 0.8071139172438091\\n2.1499999999999995 0.7817640548070683\\n2.1999999999999993 0.7849235827168971\\n2.249999999999999 0.8002323036630431\\n2.299999999999999 0.8006677849020192\\n2.3499999999999988 0.8026971693074595\\n2.3999999999999986 0.7829305179191633\\n2.4499999999999984 0.8000819070214088\\n2.4999999999999982 0.8144846512576605\\n2.549999999999998 0.7584803934346462\\n2.599999999999998 0.8094465648854962\\n2.6499999999999977 0.8028424302827774\\n2.6999999999999975 0.8026711563768261\\n2.7499999999999973 0.7883496555315228\\n2.799999999999997 0.8169983162406245\\n2.849999999999997 0.8040404040404041\\n2.899999999999997 0.7843753853742755\\n2.9499999999999966 0.7891142191142191\\n2.9999999999999964 0.7970483341584613\\n3.0499999999999963 0.7826048329779673\\n3.099999999999996 0.7996438000040306\\n3.149999999999996 0.8004216348551481\\n3.1999999999999957 0.7975259473811248\\n3.2499999999999956 0.8104505935831237\\n3.2999999999999954 0.7923782416192284\\n3.349999999999995 0.8022344996930633\\n3.399999999999995 0.8079854522454143\\n3.449999999999995 0.8289995549181544\\n3.4999999999999947 0.7932573090463799\\n3.5499999999999945 0.8008438228438228\\n3.5999999999999943 0.809004662004662\\n3.649999999999994 0.8074961878029119\\n3.699999999999994 0.816032837149498\\n3.749999999999994 0.7907520259618133\\n3.7999999999999936 0.8251874293930368\\n3.8499999999999934 0.7792741165234002\\n3.8999999999999932 0.796911421911422\\n3.949999999999993 0.8119436242798217\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in np.arange(1, 3, 0.25):\n",
    "    scores = []\n",
    "    for _ in range(1):\n",
    "        X_train, y_train, X_test, y_test = generate_data(20)\n",
    "        clf = SVC(kernel = 'rbf', C = i, gamma = 'auto', class_weight = 'balanced')\n",
    "        preds = clf.fit(X_train, y_train).predict(X_test)\n",
    "        scor = balanced_accuracy_score(y_test, preds)\n",
    "        scores.append(scor)\n",
    "    print(i, scor.mean())\n",
    "\"\"\"\n",
    "C\n",
    "2.0 0.7989526768510238\n",
    "2.05 0.7663162573389564\n",
    "2.0999999999999996 0.8071139172438091\n",
    "2.1499999999999995 0.7817640548070683\n",
    "2.1999999999999993 0.7849235827168971\n",
    "2.249999999999999 0.8002323036630431\n",
    "2.299999999999999 0.8006677849020192\n",
    "2.3499999999999988 0.8026971693074595\n",
    "2.3999999999999986 0.7829305179191633\n",
    "2.4499999999999984 0.8000819070214088\n",
    "2.4999999999999982 0.8144846512576605\n",
    "2.549999999999998 0.7584803934346462\n",
    "2.599999999999998 0.8094465648854962\n",
    "2.6499999999999977 0.8028424302827774\n",
    "2.6999999999999975 0.8026711563768261\n",
    "2.7499999999999973 0.7883496555315228\n",
    "2.799999999999997 0.8169983162406245\n",
    "2.849999999999997 0.8040404040404041\n",
    "2.899999999999997 0.7843753853742755\n",
    "2.9499999999999966 0.7891142191142191\n",
    "2.9999999999999964 0.7970483341584613\n",
    "3.0499999999999963 0.7826048329779673\n",
    "3.099999999999996 0.7996438000040306\n",
    "3.149999999999996 0.8004216348551481\n",
    "3.1999999999999957 0.7975259473811248\n",
    "3.2499999999999956 0.8104505935831237\n",
    "3.2999999999999954 0.7923782416192284\n",
    "3.349999999999995 0.8022344996930633\n",
    "3.399999999999995 0.8079854522454143\n",
    "3.449999999999995 0.8289995549181544\n",
    "3.4999999999999947 0.7932573090463799\n",
    "3.5499999999999945 0.8008438228438228\n",
    "3.5999999999999943 0.809004662004662\n",
    "3.649999999999994 0.8074961878029119\n",
    "3.699999999999994 0.816032837149498\n",
    "3.749999999999994 0.7907520259618133\n",
    "3.7999999999999936 0.8251874293930368\n",
    "3.8499999999999934 0.7792741165234002\n",
    "3.8999999999999932 0.796911421911422\n",
    "3.949999999999993 0.8119436242798217\n",
    "\n",
    "gamma\n",
    "2.0 0.7989526768510238\n",
    "2.05 0.7663162573389564\n",
    "2.0999999999999996 0.8071139172438091\n",
    "2.1499999999999995 0.7817640548070683\n",
    "2.1999999999999993 0.7849235827168971\n",
    "2.249999999999999 0.8002323036630431\n",
    "2.299999999999999 0.8006677849020192\n",
    "2.3499999999999988 0.8026971693074595\n",
    "2.3999999999999986 0.7829305179191633\n",
    "2.4499999999999984 0.8000819070214088\n",
    "2.4999999999999982 0.8144846512576605\n",
    "2.549999999999998 0.7584803934346462\n",
    "2.599999999999998 0.8094465648854962\n",
    "2.6499999999999977 0.8028424302827774\n",
    "2.6999999999999975 0.8026711563768261\n",
    "2.7499999999999973 0.7883496555315228\n",
    "2.799999999999997 0.8169983162406245\n",
    "2.849999999999997 0.8040404040404041\n",
    "2.899999999999997 0.7843753853742755\n",
    "2.9499999999999966 0.7891142191142191\n",
    "2.9999999999999964 0.7970483341584613\n",
    "3.0499999999999963 0.7826048329779673\n",
    "3.099999999999996 0.7996438000040306\n",
    "3.149999999999996 0.8004216348551481\n",
    "3.1999999999999957 0.7975259473811248\n",
    "3.2499999999999956 0.8104505935831237\n",
    "3.2999999999999954 0.7923782416192284\n",
    "3.349999999999995 0.8022344996930633\n",
    "3.399999999999995 0.8079854522454143\n",
    "3.449999999999995 0.8289995549181544\n",
    "3.4999999999999947 0.7932573090463799\n",
    "3.5499999999999945 0.8008438228438228\n",
    "3.5999999999999943 0.809004662004662\n",
    "3.649999999999994 0.8074961878029119\n",
    "3.699999999999994 0.816032837149498\n",
    "3.749999999999994 0.7907520259618133\n",
    "3.7999999999999936 0.8251874293930368\n",
    "3.8499999999999934 0.7792741165234002\n",
    "3.8999999999999932 0.796911421911422\n",
    "3.949999999999993 0.8119436242798217\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837218772269163\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = generate_data(15)\n",
    "clf = SVC(kernel = 'rbf', C = 1, gamma = 'auto', class_weight = 'balanced')\n",
    "preds = clf.fit(X_train, y_train).predict(X_test)\n",
    "print(balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussiannb_predicitons(X_train, y_train, X_test):\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data()\n",
    "preds = gaussiannb_predicitons(X_train, y_train, X_test)\n",
    "scor = balanced_accuracy_score(y_test, preds)\n",
    "print(scor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "\n",
    "def gaussianprocess_classifier(X_train, y_train, X_test):\n",
    "    model = GaussianProcessClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data()\n",
    "preds = gaussianprocess_classifier(X_train, y_train, X_test)\n",
    "print(balanced_accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-45960b9c2c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbalanced_accuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-32bfee4bb369>\u001b[0m in \u001b[0;36mgenerate_data\u001b[1;34m(percentage)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnew_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mnew_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'complex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01f5aad33bd5>\u001b[0m in \u001b[0;36mfeaturize_df\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnr_of_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnr_of_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mrow_ftrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_ftrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01f5aad33bd5>\u001b[0m in \u001b[0;36mfeaturize\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mall_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corpus'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mall_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_word_structure_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mall_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_wordnet_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[0mall_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplex_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mall_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_complexity_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01f5aad33bd5>\u001b[0m in \u001b[0;36mget_wordnet_features\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_wordnet_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;31m# try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m#     features.append(len(wordnet.synsets(word)[0].definition()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[1;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[0;32m   1603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS_LIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m             return [\n\u001b[0m\u001b[0;32m   1606\u001b[0m                 \u001b[0mget_synset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1604\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOS_LIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m             return [\n\u001b[1;32m-> 1606\u001b[1;33m                 \u001b[0mget_synset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1607\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1608\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynset_from_pos_and_offset\u001b[1;34m(self, pos, offset)\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[0mdata_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0mdata_file_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m         \u001b[1;31m# If valid, the offset equals the 8-digit 0-padded integer found at the start of the line:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0mline_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_file_line\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m             \u001b[0mstartpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[0mnew_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svm_classifier(X_train, y_train, X_test):\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_data()\n",
    "preds = svm_classifier(X_train, y_train, X_test)\n",
    "print(balanced_accuracy_score(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def lineardiscriminant_classifier(X_train, y_train, X_test):\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "\n",
    "scor = []\n",
    "for _ in range(1):\n",
    "    X_train, y_train, X_test, y_test = generate_data()\n",
    "    preds = gaussianprocess_classifier(X_train, y_train, X_test)\n",
    "    scor.append(balanced_accuracy_score(y_test, preds))\n",
    "print(cupy.array(scor).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = featurize_df(train)\n",
    "y_train = train['complex'].values\n",
    "X_test = featurize_df(test)\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test.index + len(train) + 1\n",
    "df['complex'] = knn_predicitons(X_train, y_train, X_test, neighbours=5)\n",
    "df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = featurize_df(train)\n",
    "y_train = train['complex'].values\n",
    "X_test = featurize_df(test)\n",
    "df = pd.DataFrame()\n",
    "df['id'] = test.index + len(train) + 1\n",
    "clf = SVC(kernel = 'rbf', C = 1, gamma = 'auto', class_weight = 'balanced')\n",
    "df['complex'] = clf.fit(X_train, y_train).predict(X_test)\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    }
   ],
   "source": [
    "print(submission['complex'].sum())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b455c340f5909324bcb78ef01d475f4eedf82c96c915f98f0f532100f1af7503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
